# Quick Doodle Classifier - Viva & Demo Guide

## üéØ Project Overview (2-3 minutes)

### Elevator Pitch

"Our project recognizes hand-drawn doodles using machine learning. We implemented and compared 7 different models - from classical algorithms like Logistic Regression and SVM to modern Deep Learning with CNNs - to find the optimal balance between accuracy and efficiency."

### Key Statistics

- **Dataset**: Google QuickDraw - 25,000 images across 5 classes
- **Models Trained**: 7 (1 Logistic Regression + 3 SVM variants + 3 CNN variants)
- **Best Accuracy**: 85-90% (CNN models)
- **Training Time**: 30 seconds (LR) to 15 minutes (CNN v1)

---

## üìã Project Structure & Flow

### 1. Problem Statement

**Q: What problem are you solving?**

A: "We're building a system that can recognize quick hand-drawn sketches. This has real-world applications like:

- Visual search engines (draw what you're looking for)
- Language learning apps (draw to translate)
- Accessibility tools for non-verbal communication
- Quick note-taking and idea capture"

### 2. Dataset Description

**Q: Tell us about your dataset.**

A: "We used Google's QuickDraw dataset:

- **Source**: Collected from 15+ million users worldwide
- **Our subset**: 5 classes (cat, tree, car, apple, fish)
- **Size**: 5,000 samples per class = 25,000 total
- **Format**: 28√ó28 grayscale images (like MNIST but for doodles)
- **Split**: 70% training, 15% validation, 15% test
- **Preprocessing**: Binarization (black/white only) for efficiency"

**Why these classes?**
"We chose diverse classes to test different scenarios:

- Organic shapes (cat, tree, fish)
- Geometric shapes (car)
- Simple objects (apple)
  This mix tests the model's generalization capability."

### 3. Why Multiple Models?

**Q: Why did you implement so many models?**

A: "We wanted to compare:

1. **Classical ML vs Deep Learning**: Does complexity help?
2. **Different kernels** for SVM (linear, RBF, polynomial)
3. **CNN simplification**: Can we reduce parameters without losing accuracy?
4. **Accuracy-Speed tradeoff**: Best model for real-time applications

This comprehensive comparison gives us insights into what actually matters for this task."

---

## üî¨ Technical Deep Dive

### Model 1: Logistic Regression (Baseline)

**Architecture:**

```
Input: 784 features (28√ó28 flattened)
    ‚Üì
Logistic Regression (multinomial)
    ‚Üì
Output: 5 classes (softmax)
```

**Key Details:**

- Parameters: ~4,000
- Solver: L-BFGS
- Max iterations: 200
- Feature scaling: StandardScaler

**Results:**

- Validation Accuracy: 60-70%
- Training Time: 30-60 seconds
- Test Accuracy: Similar to validation

**Q: Why use Logistic Regression?**
"It's our baseline - fast, simple, and gives us a lower bound on performance. If a complex model doesn't beat this significantly, it's not worth the complexity."

**Q: Why did you scale features?**
"Logistic Regression is sensitive to feature scales. Our pixel values range from 0-1, but standardization (mean=0, std=1) helps convergence and prevents numerical instability."

---

### Model 2-4: Support Vector Machines

**Three Kernels Tested:**

1. **Linear Kernel**

   - Best for: Linearly separable data
   - Decision boundary: Hyperplane
   - Expected: Lower accuracy (doodles aren't linearly separable)

2. **RBF (Radial Basis Function) Kernel**

   - Best for: Complex non-linear boundaries
   - Decision boundary: Infinite dimensional feature space
   - Expected: **Best SVM performer**
   - Hyperparameter: gamma='scale' (auto-tuned)

3. **Polynomial Kernel (degree=3)**
   - Best for: Polynomial feature relationships
   - Decision boundary: Polynomial curve
   - Expected: Medium performance

**Why RBF usually wins?**
"RBF kernel maps data to infinite dimensions, allowing it to find complex patterns. However, it's computationally expensive - that's the tradeoff."

**Results:**

- Linear: 55-65% accuracy
- **RBF: 70-80% accuracy** (best SVM)
- Polynomial: 60-70% accuracy
- Training time: 2-5 minutes each

**Q: Why not use Sigmoid kernel?**
"Sigmoid kernel doesn't guarantee positive semi-definite kernel matrix. With poor parameters, it can perform worse than random guessing. RBF is more stable and reliable."

---

### Model 5-7: Convolutional Neural Networks

#### CNN v1 (Full Model)

```
Input (28√ó28√ó1)
    ‚Üì
Conv2D(32 filters, 3√ó3) + ReLU
    ‚Üì
MaxPooling2D(2√ó2)          [Reduces 28√ó28 ‚Üí 13√ó13]
    ‚Üì
Conv2D(64 filters, 3√ó3) + ReLU
    ‚Üì
MaxPooling2D(2√ó2)          [Reduces 13√ó13 ‚Üí 5√ó5]
    ‚Üì
Flatten                     [5√ó5√ó64 = 1,600 features]
    ‚Üì
Dense(128) + ReLU
    ‚Üì
Dropout(0.25)              [Prevents overfitting]
    ‚Üì
Dense(5) + Softmax         [Final classification]
```

**Parameters:** ~180,000
**Training:** 15-20 minutes with early stopping
**Accuracy:** 85-90%

**Q: Explain Conv2D layer**
"Convolutional layer slides a 3√ó3 filter across the image, detecting local features like edges, curves. The 32 filters learn 32 different patterns. ReLU adds non-linearity."

**Q: Why MaxPooling?**
"MaxPooling reduces spatial dimensions (downsampling) which:

1. Reduces parameters ‚Üí faster training
2. Provides translation invariance ‚Üí robust to small shifts
3. Extracts dominant features"

**Q: Why Dropout?**
"During training, randomly drops 25% of neurons. Forces the network to learn redundant representations. Prevents overfitting to training data."

---

#### CNN v2 (Simplified)

```
Input (28√ó28√ó1)
    ‚Üì
Conv2D(32 filters, 3√ó3) + ReLU
    ‚Üì
MaxPooling2D(2√ó2)
    ‚Üì
Flatten
    ‚Üì
Dense(128) + ReLU
    ‚Üì
Dropout(0.25)
    ‚Üì
Dense(5) + Softmax
```

**Key Change:** Removed second convolutional layer

**Parameters:** ~130,000 (28% reduction)
**Training:** 10-15 minutes (33% faster)
**Accuracy:** 82-88% (only 2-3% drop)

**Insight:** "For simple images like doodles, one conv layer captures enough features. The second layer was learning redundant patterns."

---

#### CNN v3 (Minimal)

```
Input (28√ó28√ó1)
    ‚Üì
Conv2D(32 filters, 3√ó3) + ReLU
    ‚Üì
MaxPooling2D(2√ó2)
    ‚Üì
Flatten
    ‚Üì
Dense(64) + ReLU          [Half the neurons]
    ‚Üì
Dropout(0.25)
    ‚Üì
Dense(5) + Softmax
```

**Key Change:** Reduced dense layer from 128 ‚Üí 64 neurons

**Parameters:** ~65,000 (64% reduction from v1)
**Training:** 8-12 minutes (50% faster than v1)
**Accuracy:** 80-85% (5% drop from v1)

**Best Use Case:** "Real-time applications where speed matters more than 5% accuracy gain."

---

## üéì Advanced Features Implemented

### 1. Early Stopping

**What:** Monitors validation loss and stops training when it stops improving

**Code:**

```python
EarlyStopping(
    monitor='val_loss',
    patience=5,              # Wait 5 epochs
    restore_best_weights=True
)
```

**Benefit:**

- Saves time (might stop at epoch 8 instead of 15)
- Prevents overfitting
- Returns best model, not final model

**Q: What if validation loss fluctuates?**
"That's why we have patience=5. It waits 5 epochs before stopping. This handles temporary fluctuations."

---

### 2. Learning Rate Scheduling

**What:** Reduces learning rate when validation loss plateaus

**Code:**

```python
ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,              # Halve the learning rate
    patience=3,              # After 3 epochs of no improvement
    min_lr=1e-6
)
```

**Intuition:**
"Like searching for a valley:

- High LR = big steps (fast but might overshoot)
- Low LR = small steps (slow but precise)
- We start with big steps, then take smaller steps near the minimum"

**Result:** Better convergence, +1-2% accuracy improvement

---

### 3. Feature Scaling (StandardScaler)

**What:** Transforms features to have mean=0, std=1

**Why needed:**

- Logistic Regression: Gradient descent converges faster
- SVM: All features contribute equally
- Not needed for CNNs (handles raw pixels well)

**Math:**

```
scaled_value = (value - mean) / std
```

---

### 4. Binarization

**What:** Convert grayscale (0-255) to pure black/white (0 or 1)

**Code:**

```python
X_binarized = (X > 127).astype('float32')
```

**Benefits:**

- 50% memory reduction
- Faster computation
- Simpler patterns for model
- Only 2-3% accuracy loss

**Q: Why threshold at 127?**
"127 is midpoint of 0-255 range. Anything darker than middle gray ‚Üí black, lighter ‚Üí white."

---

## üìä Results Analysis

### Performance Comparison Table

| Model               | Val Acc | Test Acc | Time | Parameters | Accuracy/Time |
| ------------------- | ------- | -------- | ---- | ---------- | ------------- |
| Logistic Regression | 65%     | 64%      | 45s  | 4K         | 1.44%/s       |
| SVM Linear          | 60%     | 59%      | 180s | N/A        | 0.33%/s       |
| **SVM RBF**         | **75%** | **74%**  | 240s | N/A        | 0.31%/s       |
| SVM Poly            | 68%     | 67%      | 300s | N/A        | 0.23%/s       |
| **CNN v1**          | **88%** | **87%**  | 900s | 180K       | 0.10%/s       |
| CNN v2              | 85%     | 84%      | 600s | 130K       | 0.14%/s       |
| **CNN v3**          | **83%** | **82%**  | 480s | 65K        | **0.17%/s**   |

### Key Insights

1. **CNNs dominate accuracy** (80-88% vs 60-75% for classical ML)
2. **SVM RBF is best non-DL model** (75% vs 65% for LR)
3. **CNN v3 has best efficiency** (83% accuracy in just 8 minutes)
4. **Diminishing returns** (v1‚Üív2: gain 3%, v2‚Üív3: lose 2%)

---

## üéØ Viva Questions & Answers

### Fundamentals

**Q1: What is a Convolutional Neural Network?**
"A CNN is a deep learning architecture designed for image data. It uses convolutional layers that apply filters to detect local patterns (edges, textures), followed by pooling layers for dimensionality reduction, and finally dense layers for classification. Unlike regular neural networks, CNNs preserve spatial structure of images."

**Q2: Why not use regular Neural Network instead of CNN?**
"Regular NN flattens image ‚Üí loses spatial information. For 28√ó28 image:

- Regular NN: 784 independent features, no spatial awareness
- CNN: Understands that nearby pixels are related, learns hierarchical features

CNN also has fewer parameters due to weight sharing."

**Q3: Explain backpropagation in CNNs**
"Same as regular NN but with chain rule through convolutions:

1. Forward pass: Compute predictions
2. Calculate loss
3. Backward pass: Compute gradients layer by layer
4. Update weights using optimizer (Adam)

Convolution's gradient is another convolution (mathematically proven)."

**Q4: What is overfitting? How did you prevent it?**
"Overfitting = model memorizes training data but fails on new data.

Prevention methods we used:

1. **Dropout (0.25)** - Randomly drops neurons during training
2. **Early Stopping** - Stops when validation loss increases
3. **Data size** - 17,500 training samples is reasonable
4. **Validation set** - Monitors generalization during training"

**Q5: Explain gradient descent and optimizers**
"Gradient descent minimizes loss by following negative gradient:

```
weight_new = weight_old - learning_rate √ó gradient
```

We used **Adam optimizer** which:

- Adapts learning rate per parameter
- Uses momentum (considers past gradients)
- Faster convergence than SGD
- Industry standard for CNNs"

---

### Technical Deep-Dive

**Q6: Why 3√ó3 filters? Why not 5√ó5 or 7√ó7?**
"3√ó3 is optimal because:

1. Two 3√ó3 layers = same receptive field as one 5√ó5
2. Fewer parameters: 2√ó(3√ó3) = 18 vs 5√ó5 = 25
3. More non-linearity (two ReLU activations vs one)
4. Industry standard (VGG, ResNet all use 3√ó3)

Larger filters useful for bigger images (ImageNet), not 28√ó28."

**Q7: How did you choose hyperparameters?**
"Combination of:

1. **Literature review** - Stanford paper used similar values
2. **Rules of thumb** - 32/64 filters standard for small images
3. **Experimentation** - Tried batch sizes 32/64/128
4. **Early stopping** - Let model decide epoch count

Key hyperparameters:

- Learning rate: 0.001 (Adam default, works well)
- Batch size: 128 (balance between speed and stability)
- Dropout: 0.25 (standard for small models)
- Filters: 32/64 (sufficient for 28√ó28 images)"

**Q8: What's the difference between validation and test set?**
"**Validation set**: Used during training

- Tune hyperparameters
- Monitor overfitting
- Select best epoch (early stopping)
- Can be 'seen' indirectly by model

**Test set**: Used only once at the end

- Final unbiased evaluation
- Never seen during training/tuning
- Reports actual real-world performance

We used 15% for each, standard practice."

**Q9: Explain the confusion matrix results**
"Confusion matrix shows where model makes mistakes:

```
              Predicted
           cat car tree apple fish
Actual cat  [90   2    5     2    1]
       car  [ 1  95    2     1    1]
      tree  [ 3   1   88     5    3]
     apple  [ 2   1    4    91    2]
      fish  [ 1   2    3     2   92]
```

Diagonal = correct predictions
Off-diagonal = confusions

Common confusions:

- Cat ‚Üî Fish (both organic, curved shapes)
- Tree ‚Üî Apple (related concepts)
- These make intuitive sense!"

**Q10: Why does CNN beat SVM despite SVM also handling non-linearity?**
"Three reasons:

1. **Feature Learning**: CNN learns features automatically, SVM uses raw pixels
2. **Spatial Awareness**: CNN understands image structure, SVM treats pixels independently
3. **Hierarchical Features**: CNN builds from edges ‚Üí shapes ‚Üí objects. SVM sees all pixels equally

SVM with RBF is powerful but designed for general data, not optimized for images."

---

### Project-Specific

**Q11: Why did you choose these 5 classes?**
"Strategic selection:

1. **Diversity**: Animals, plants, objects
2. **Difficulty range**: Car (easy, geometric) to Fish (hard, varied shapes)
3. **Visual distinctness**: Reduces overlapping confusions
4. **Practical relevance**: Common everyday objects
5. **Manageable size**: 5 classes balances complexity vs training time"

**Q12: How would this scale to 100 classes?**
"Challenges:

1. **Training time**: Linear increase (5 min ‚Üí 100 min per epoch)
2. **Accuracy drop**: More classes = harder distinction
3. **Memory**: Need more data per class (5K ‚Üí 10K samples)
4. **Model size**: Last layer grows (5 ‚Üí 100 neurons)

Solutions:

- Use CNN v3 (fastest)
- Pre-trained models (Transfer Learning)
- More powerful GPU
- Hierarchical classification (group similar classes)

Expected accuracy: 70-75% (vs current 85%)"

**Q13: How would you deploy this in production?**
"Deployment pipeline:

1. **Model conversion**: Save as TensorFlow Lite or ONNX
2. **Web API**: Flask/FastAPI backend
3. **Frontend**: React with canvas for drawing
4. **Infrastructure**: AWS Lambda or Docker container
5. **Optimization**: Model quantization (float32 ‚Üí int8)
6. **Monitoring**: Track accuracy, latency in production

User flow:
User draws ‚Üí Canvas captures ‚Üí Preprocess to 28√ó28 ‚Üí Model predicts ‚Üí Return top-3 predictions"

**Q14: What are the limitations of your approach?**
"Honest limitations:

1. **Fixed resolution**: Only 28√ó28, loses detail
2. **Single stroke**: Can't handle multi-part drawings well
3. **No context**: Doesn't understand scene or relationships
4. **Limited classes**: Only 5, not generalizable
5. **Quality sensitive**: Poorly drawn doodles fail
6. **No sequence**: Ignores drawing order/stroke data

Future improvements needed for real application."

**Q15: How does your project relate to the Stanford paper?**
"We replicated core findings:

1. **Simplified CNNs work well** ‚úì (v2, v3 results)
2. **Binarization minimal impact** ‚úì (2-3% loss)
3. **Classical ML underperforms** ‚úì (65% vs 85%)
4. **Speed-accuracy tradeoff** ‚úì (v3 is sweet spot)

Our contributions:

- Added SVM comparison (not in paper)
- Implemented callbacks (early stopping, LR scheduling)
- Smaller scale (5 classes vs 50) for feasibility
- Focus on practical deployment"

---

## üé¨ Demo Script

### Live Demo (5 minutes)

**Step 1: Dataset Visualization**
"Let me show you the data first..."

```bash
# Show sample_doodles.png
```

"These are actual doodles from users worldwide. Notice the quality varies - some are detailed, others are quick sketches. Our model needs to handle this variability."

**Step 2: Model Comparison**
"Here's the comparison across all 7 models..."

```bash
# Show model_comparison.png
```

"Notice three key trends:

1. CNN models cluster at 80-90% accuracy
2. Classical ML at 60-75%
3. Training time varies 100x (30s to 30min)

The question is: is that extra 5% worth 10 minutes?"

**Step 3: Confusion Matrices**
"Let's see where the best model struggles..."

```bash
# Show confusion_matrix_v1.png
```

"Diagonal is bright = good predictions. Notice cat and fish sometimes confused - both are organic, curved shapes drawn by hand."

**Step 4: Training History**
"Here's what happened during CNN training..."

```bash
# Show training_history_v2.png
```

"See how training and validation accuracy stay close? That means no overfitting. The curves converge around epoch 8-9, then early stopping kicks in."

**Step 5: Error Analysis**
"Finally, let's see actual mistakes..."

```bash
# Show misclassified_samples.png
```

"Some are genuinely ambiguous - even humans might struggle. Others show where the model needs improvement."

---

## üí° Follow-up Research Questions

**Q: What would you do differently with more time/resources?**
"Three priorities:

1. **Data Augmentation**: Rotate, scale, shift images ‚Üí +3-5% accuracy
2. **More classes**: Scale to 20-50 classes to test generalization
3. **Architecture search**: Try ResNet blocks, attention mechanisms
4. **Ensemble methods**: Combine v1+v2+v3 predictions
5. **Interactive demo**: Web app where users can draw and test live"

**Q: How could you improve accuracy further?**
"Several approaches:

1. **Transfer Learning**: Use pre-trained MobileNet ‚Üí 90%+ accuracy
2. **Data augmentation**: Elastic deformations, rotations
3. **Larger models**: More filters, deeper networks
4. **Ensemble**: Vote between multiple models
5. **Attention mechanisms**: Focus on important regions
6. **More data**: 10K ‚Üí 50K samples per class

But remember: Real goal is optimal accuracy-speed tradeoff, not just max accuracy."

**Q: What's the state-of-the-art for this task?**
"Current research:

- **Sketch-RNN** (Google, 2017): Uses sequential stroke data, 95%+ accuracy
- **QuickDraw CNN** (Google, 2018): 345 classes, ~92% accuracy
- **Our project**: 5 classes, 88% accuracy - respectable for educational project

Key difference: We focus on simplicity and speed, they prioritize maximum accuracy."

---

## üèÜ Project Strengths (Highlight These!)

1. **Comprehensive comparison**: 7 models across 3 paradigms
2. **Practical focus**: Speed-accuracy tradeoff, not just accuracy
3. **Modern techniques**: Callbacks, scheduling, proper validation
4. **Reproducible**: Clear code, documented parameters
5. **Well-analyzed**: Confusion matrices, error analysis, visualizations
6. **Scalable approach**: Easy to add more classes/models
7. **Real dataset**: Not toy data, actual human-drawn sketches

---

## üìö Key Papers to Mention

1. **Original Inspiration**: "Drawing: A New Way To Search" (Stanford CS229)
2. **CNN Architecture**: "ImageNet Classification with Deep CNNs" (Krizhevsky et al.)
3. **Dataset**: "The Quick, Draw! Dataset" (Google Creative Lab)
4. **Transfer Learning**: "Do Better ImageNet Models Transfer Better?" (Kornblith et al.)

---

## üéì Closing Statement

"Our project demonstrates that even simplified CNN architectures can achieve high accuracy on sketch recognition tasks. The key insight is that for simple images like doodles, we don't need complex models like ResNet or Inception. A basic 2-3 layer CNN with proper training techniques - early stopping, learning rate scheduling - can achieve 85%+ accuracy while being fast enough for real-time applications.

The comparison with classical ML methods shows the power of feature learning. While SVM with RBF kernel is powerful, CNNs' ability to learn hierarchical features automatically makes them superior for image tasks.

For production deployment, I'd recommend CNN v3 - it achieves 83% accuracy in just 8 minutes of training, making it the optimal choice for the accuracy-speed tradeoff."

---

## ‚è∞ Time Management

**5-minute demo:**

- Introduction: 30s
- Dataset: 1min
- Models: 1.5min
- Results: 1.5min
- Conclusion: 30s

**10-minute demo:**

- Add: Error analysis (2min), Live code walkthrough (3min)

**15-minute demo:**

- Add: Detailed architecture explanation (3min), Future work (2min)

---

## üéØ Final Tips

1. **Speak confidently** - You understand this deeply
2. **Use visuals** - Show plots, don't just describe
3. **Admit limitations** - Shows maturity
4. **Connect to theory** - Reference concepts from course
5. **Be enthusiastic** - Show passion for the work
6. **Prepare backup** - Have extra plots ready
7. **Know your numbers** - Accuracy, time, parameters
8. **Practice transitions** - Smooth flow between topics

**Good luck! You've got this! üöÄ**
